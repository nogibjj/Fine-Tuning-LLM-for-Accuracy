{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From HuggingFace TRL Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 665/665 [00:00<00:00, 1.82MB/s]\n",
      "model.safetensors: 100%|██████████| 548M/548M [00:07<00:00, 68.8MB/s]\n",
      "generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 520kB/s]\n",
      "pytorch_model.bin: 100%|██████████| 548M/548M [00:03<00:00, 144MB/s] \n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 32.3MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 76.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 38.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "# load a pretrained model\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\n",
    "model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:257: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# initialize trainer\n",
    "ppo_config = {\"batch_size\": 1}\n",
    "config = PPOConfig(**ppo_config)\n",
    "ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode a query\n",
    "query_txt = \"\"\"Please provide the answer to the below prompt and respond with how confident you are that your response is accurate.  \n",
    "For example:\n",
    "Prompt:\n",
    "1 + 1 = ?\n",
    "Response:\n",
    "I am very confident that 1 + 1 = 2\n",
    "Prompt:\n",
    "Who will be the next president?\n",
    "Response:\n",
    "I am unsure who will be the next president, but incumbents are most likely to win so I expect Joe Biden will be the next president.\n",
    "Prompt:\n",
    "10 + 10 = ?\n",
    "Response:\"\"\"\n",
    "query_tensor = tokenizer.encode(query_txt, return_tensors=\"pt\").to(model.pretrained_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate model response\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 20,\n",
    "}\n",
    "response_tensor = ppo_trainer.generate([item for item in query_tensor], return_prompt=False, **generation_kwargs)\n",
    "response_txt = tokenizer.decode(response_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI am very confident that 10 + 10 = 8 = 12 =?\\n\\n10+6'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a reward for response\n",
    "# (this could be any reward such as human feedback or output from another model)\n",
    "reward = [torch.tensor(1.0, device=model.pretrained_model.device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model with ppo\n",
    "train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective/kl': 0.0,\n",
       " 'objective/kl_dist': 0.0,\n",
       " 'objective/logprobs': array([[-8.146128  , -2.2490625 , -2.660843  , -0.63041306, -1.6609842 ,\n",
       "         -9.227812  , -3.3473172 , -5.8727584 , -3.7109416 , -0.02053279,\n",
       "         -6.558813  , -1.7685627 , -6.5495687 , -2.253131  , -3.993826  ,\n",
       "         -1.3066202 , -1.3901442 , -0.4830762 , -0.46233207, -5.5269938 ,\n",
       "         -1.3268279 , -2.02606   , -7.9535437 , -0.25260922, -3.7766173 ,\n",
       "         -5.2870355 ]], dtype=float32),\n",
       " 'objective/ref_logprobs': array([[-8.146128  , -2.2490625 , -2.660843  , -0.63041306, -1.6609842 ,\n",
       "         -9.227812  , -3.3473172 , -5.8727584 , -3.7109416 , -0.02053279,\n",
       "         -6.558813  , -1.7685627 , -6.5495687 , -2.253131  , -3.993826  ,\n",
       "         -1.3066202 , -1.3901442 , -0.4830762 , -0.46233207, -5.5269938 ,\n",
       "         -1.3268279 , -2.02606   , -7.9535437 , -0.25260922, -3.7766173 ,\n",
       "         -5.2870355 ]], dtype=float32),\n",
       " 'objective/kl_coef': 0.2,\n",
       " 'objective/entropy': 63.867313385009766,\n",
       " 'ppo/mean_non_score_reward': 0.0,\n",
       " 'ppo/mean_scores': 1.0,\n",
       " 'ppo/std_scores': nan,\n",
       " 'tokens/queries_len_mean': 7.0,\n",
       " 'tokens/queries_len_std': nan,\n",
       " 'tokens/queries_dist': 7.0,\n",
       " 'tokens/responses_len_mean': 20.0,\n",
       " 'tokens/responses_len_std': nan,\n",
       " 'tokens/responses_dist': 20.0,\n",
       " 'ppo/loss/policy': -0.09400573372840881,\n",
       " 'ppo/loss/value': 8.962589263916016,\n",
       " 'ppo/loss/total': 0.8022533059120178,\n",
       " 'ppo/policy/entropy': 3.379899024963379,\n",
       " 'ppo/policy/approxkl': 0.16758942604064941,\n",
       " 'ppo/policy/policykl': -0.0032557332888245583,\n",
       " 'ppo/policy/clipfrac': 0.4375,\n",
       " 'ppo/policy/advantages': array([ 3.6695924 ,  3.7648878 ,  3.8651993 ,  3.9707897 ,  4.0819373 ,\n",
       "         4.1989355 ,  1.292051  ,  1.2634641 ,  1.1828254 ,  2.0659316 ,\n",
       "         0.6946371 ,  0.4735545 , -0.19248545,  0.06785427, -0.24879013,\n",
       "        -0.57772285,  0.6746595 , -0.12883125,  0.20833695, -0.9180146 ,\n",
       "        -0.5029876 , -1.073797  , -1.335102  , -0.04654355, -1.7429312 ,\n",
       "        -1.1561122 ,  3.6695924 ,  3.7648878 ,  3.8651993 ,  3.9707897 ,\n",
       "         4.0819373 ,  4.1989355 ,  1.292051  ,  1.2634641 ,  1.1828254 ,\n",
       "         2.0659316 ,  0.6946371 ,  0.4735545 , -0.19248545,  0.06785427,\n",
       "        -0.24879013, -0.57772285,  0.6746595 , -0.12883125,  0.20833695,\n",
       "        -0.9180146 , -0.5029876 , -1.073797  , -1.335102  , -0.04654355,\n",
       "        -1.7429312 , -1.1561122 ,  3.6695924 ,  3.7648878 ,  3.8651993 ,\n",
       "         3.9707897 ,  4.0819373 ,  4.1989355 ,  1.292051  ,  1.2634641 ,\n",
       "         1.1828254 ,  2.0659316 ,  0.6946371 ,  0.4735545 , -0.19248545,\n",
       "         0.06785427, -0.24879013, -0.57772285,  0.6746595 , -0.12883125,\n",
       "         0.20833695, -0.9180146 , -0.5029876 , -1.073797  , -1.335102  ,\n",
       "        -0.04654355, -1.7429312 , -1.1561122 ,  3.6695924 ,  3.7648878 ,\n",
       "         3.8651993 ,  3.9707897 ,  4.0819373 ,  4.1989355 ,  1.292051  ,\n",
       "         1.2634641 ,  1.1828254 ,  2.0659316 ,  0.6946371 ,  0.4735545 ,\n",
       "        -0.19248545,  0.06785427, -0.24879013, -0.57772285,  0.6746595 ,\n",
       "        -0.12883125,  0.20833695, -0.9180146 , -0.5029876 , -1.073797  ,\n",
       "        -1.335102  , -0.04654355, -1.7429312 , -1.1561122 ], dtype=float32),\n",
       " 'ppo/policy/advantages_mean': -1.7881393432617188e-07,\n",
       " 'ppo/policy/ratio': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.9813188 , 1.0290327 , 1.0486368 , 1.0077091 ,\n",
       "        1.0159034 , 0.96473575, 2.8684683 , 1.3023063 , 2.256092  ,\n",
       "        1.0027355 , 1.3024684 , 1.067564  , 1.00602   , 1.032916  ,\n",
       "        0.7266335 , 0.828388  , 1.2991774 , 1.0431802 , 1.0605419 ,\n",
       "        0.84093964, 0.9197017 , 0.9578027 , 0.6891539 , 1.0405582 ,\n",
       "        0.7655552 , 0.37047085, 0.96937007, 1.0463674 , 1.0749838 ,\n",
       "        1.0116662 , 1.0214001 , 0.9458424 , 5.486633  , 1.5247372 ,\n",
       "        3.5096307 , 1.0044969 , 1.5277196 , 1.1358514 , 0.9713078 ,\n",
       "        1.0616912 , 0.5654307 , 0.59891677, 1.4810679 , 1.0681317 ,\n",
       "        1.1025535 , 0.688966  , 0.79608107, 0.86742586, 0.54682666,\n",
       "        1.0653734 , 0.64234936, 0.20474724, 0.9610745 , 1.0593125 ,\n",
       "        1.0922556 , 1.0138401 , 1.0237786 , 0.9305304 , 8.571709  ,\n",
       "        1.7101849 , 4.6826925 , 1.0057908 , 1.721328  , 1.2101034 ,\n",
       "        0.9209814 , 1.0876331 , 0.46756777, 0.45090938, 1.622217  ,\n",
       "        1.0794272 , 1.1351154 , 0.59425896, 0.706222  , 0.7605994 ,\n",
       "        0.4579574 , 1.0827491 , 0.5585588 , 0.13441938], dtype=float32),\n",
       " 'ppo/returns/mean': 3.332977771759033,\n",
       " 'ppo/returns/var': 1.512957215309143,\n",
       " 'ppo/val/vpred': 6.775060653686523,\n",
       " 'ppo/val/error': 15.758296966552734,\n",
       " 'ppo/val/clipfrac': 0.6625000238418579,\n",
       " 'ppo/val/mean': 7.197945594787598,\n",
       " 'ppo/val/var': 1.574478030204773,\n",
       " 'ppo/val/var_explained': -9.415559768676758,\n",
       " 'ppo/learning_rate': 1e-05,\n",
       " 'time/ppo/forward_pass': 0.6705300807952881,\n",
       " 'time/ppo/compute_rewards': 0.00019884109497070312,\n",
       " 'time/ppo/compute_advantages': 0.0015358924865722656,\n",
       " 'time/ppo/optimize_step': 6.407001733779907,\n",
       " 'time/ppo/calc_stats': 0.1264476776123047,\n",
       " 'time/ppo/total': 7.266981601715088}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
